* Fine-tuning GPT-3 to generate puns
** Aims
- Train GPT-3 to continue on sequences of puns
- Train GPT-3 to speak in puns

** Training
*** Data
| format  |
|---------|
| =jsonl= |

**** Sources
- https://github.com/taivop/joke-dataset

**** Naive approach
#+BEGIN_SRC python -n :i python3.6 :async :results verbatim code
  [{"data" : "joke set 1"},
   {"data": "joke set 2"}
#+END_SRC

**** Better approach
+ =text_list= :: One element resembles a _prompt_ and the other its _completion_.

The prompt is a set of jokes followed by more set of jokes of the same type.

+ Data sets
  - =train_puns.jsonl=
  - =test_puns.jsonl=

#+BEGIN_SRC python -n :i python3.6 :async :results verbatim code
  [{'data': {'text_list': ["Starting of joke set 1", "Remaining of joke set 1"], 'loss_weights': [0.3, 0.7]}},
   {'data': {'text_list': ['Starting of joke set 2", "Remaining of joke set 2"], 'loss_weights': [0.3, 0.7]}}]
#+END_SRC

The loss weights are set such that the
completion is given more importance (=0.7=)
than the prompt (=0.3=).

***** Prompt
A random number of jokes.

#+BEGIN_SRC text -n :async :results verbatim code
  What do you call a guy with no arms and no legs sitting at your doorstep? Matt.
  What do you call a guy with no arms or legs floating in your pool? Bob.
  What do you call a guy with no arms or legs hanging off your wall? Art.
#+END_SRC

***** Completion
#+BEGIN_SRC text -n :async :results verbatim code
  What do you call somebody with no body and no nose? Nobody knows
  What do you call a guy with no arms or legs? Names.
  What do you call a cow with no legs? Ground Beef. 
#+END_SRC

***** Training command
#+BEGIN_SRC bash -n :i bash :async :results verbatim code
  openai-ft \
      -t data/train_puns.jsonl \
      --val data/test_puns.jsonl \
      --num-epochs 1 \
      -e $(myrc .openai_training_engine_name) \
      -m ada \
      --batch-size 2 \
      --val-batch-size 1 \
      --snapshots-every 15 \
      --num-completions 1 \
      --completions-every 2 \
      -s 0.01 \
      --log-path logs \
      -v
#+END_SRC

***** Hyperparameters
#+BEGIN_SRC sh -n :sps bash :async :results none
  v +/"I initially trained it" "$MYGIT/cabhijith/GPT-3_Docs/examples_finetuning/harry.md"
#+END_SRC

+ TODO
  - Test different learning rates

To avoid over-fitting (using the same characters), using a lower learning rate (i.e. =0.01=).

** TODO Testing
List prompt and completion.

** Application
*** Size of dataset
#+BEGIN_SRC bash -n :i bash :async :results verbatim code
  cat $MYGIT/taivop/joke-dataset/reddit_jokes.json | jq -r 'length'
  cat $MYGIT/taivop/joke-dataset/stupidstuff.json | jq -r 'length'
  cat $MYGIT/taivop/joke-dataset/wocka.json | jq -r 'length'
#+END_SRC

#+RESULTS:
#+begin_src bash
194553
3773
10019
#+end_src

*** Average length of each joke
#+BEGIN_SRC bash -n :i bash :async :results verbatim code
  cat $MYGIT/taivop/joke-dataset/reddit_jokes.json | jq -r '.[].body|length'| jq -s add/length
#+END_SRC

#+RESULTS:
#+begin_src bashg
204.60585547382976
#+end_src

*** Email address
=mullikine@gmail.com=

*** Name
Shane Mulligan

*** Please tell us about your product (or use-case)
I am working on an emacs mode for prompt-
engineering and training GPT-3 with the OpenAI
API. I would also like to train GPT-3 to speak
in puns.
- https://github.com/mullikine/pen.el
- http://github.com/mullikine/fine-tuning-gpt-3

*** What is the task that you want to fine-tune? *
#+BEGIN_SRC text -n :async :results verbatim code
  Generative: can't fit enough examples in the prompt
  Other: I would like to automate the fine-
  tuning process in emacs. I would also like to
  get GPT-3 to generate puns after seeing some
  examples of them. I would also like to train
  GPT-3 to speak in puns.    
#+END_SRC

*** Why do you require fine-tuning for this use-case?
The completions I am getting from the API are
repetetive and do not contain puns.

I would like an engine so that I can fine-tune
GPT-3 to produce puns, and in that process
develop tooling to do more fine-tuning.

- https://github.com/mullikine/pen.el
- https://github.com/mullikine/fine-tuning-gpt-3

*** How many examples do you have and what is the typical number of words per example? *
n examples: 200000,  average words per example: 40

*** What is the size of the dataset (MB)?
2.5

*** What performance needs to be met in order to move to production with the fine-tuned model? *
I will NOT be moving to production.

*** What are the latency requirements for the fine-tuned model?
No requirements. It can be high latency, I don't mind.

*** What is the expected request volume?
100 requests per day (for myself).

*** How many different fine-tuned models would you like to use?
2

*** Security constraints?
No. I will not be applying it any sensitive information.